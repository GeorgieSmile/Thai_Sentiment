from fastapi import APIRouter, HTTPException
from fastapi import UploadFile, File
from utils.predictor import predict_sentiment
from pydantic import BaseModel
from utils.youtube_comment_fetcher import fetch_comments
import pandas as pd
import io

router = APIRouter()

# Pydantic Models 

# Single text request
class SentimentRequest(BaseModel):
    text: str

# Probability structure for each sentiment label
class Probabilities(BaseModel):
    label: str
    probability: float

# Full response for a single text
class SentimentResponse(BaseModel):
    text: str
    sentiment: str
    probabilities: list[Probabilities]
    summary: str

# Request body for multiple texts
class MultipleSentimentRequest(BaseModel):
    texts: list[str]

# Response for multiple texts
class MultipleSentimentResponse(BaseModel):
    result: list[SentimentResponse]

# YouTube comment sentiment request
class YouTubeRequest(BaseModel):
    url: str

# Utility Function
async def extract_text_from_file(file: UploadFile):
    """
    Extract text from a file.
    
    Args:
        file (UploadFile): The uploaded file.
        
    Returns:
        str: List of extracted text.
    """
    contents = await file.read()
    decoded = contents.decode("utf-8")
    texts = []

    if file.filename.endswith('.txt'):
        for line in decoded.splitlines():
            line = line.strip()
            if line:
                texts.append(line)

    elif file.filename.endswith('.csv'):
        df = pd.read_csv(io.StringIO(decoded))
        if 'text' not in df.columns:
            raise ValueError("CSV ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'text'")
        for text in df['text'].dropna().tolist():
            text = str(text).strip()
            if text:
                texts.append(text)

    else:
        raise ValueError("‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå .txt and .csv ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
    
    return texts

#API Endpoints

@router.post("/predict")
def predict(request: SentimentRequest):
    """
    Predict sentiment of a single text input.
    """

    if not request.text.strip():
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÑ‡∏î‡πâ")
    if len(request.text) > 400:
        raise HTTPException(status_code=400, 
                            detail=f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ({len(request.text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£). ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ 400 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£.")
    
    # Run prediction
    sentiment_idx, probabilities = predict_sentiment(request.text)
    labels = ["‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö üò°", "‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á üòê", "‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å üòÑ"]
    sentiment = labels[sentiment_idx]
    confidence = round(max(probabilities[0]) * 100, 2)

    # Create probabity object
    prob_object = []
    for i in range(len(probabilities[0])):
        label = labels[i]
        prob_object.append(Probabilities(label=label, probability=round(probabilities[0][i], 3)))

    summary = f"‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {confidence:.1f}% ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°{sentiment}"
    return SentimentResponse(text=request.text, sentiment=sentiment, probabilities=prob_object, summary=summary)

@router.post("/predict_multiple")
def predict_multiple(request: MultipleSentimentRequest):
    """
    Predict sentiment for a list of text inputs (max 250).
    """
    texts_to_process = request.texts[:250]
    if not texts_to_process:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÑ‡∏î‡πâ")
    if any(len(text) > 400 for text in texts_to_process):
        raise HTTPException(status_code=400, 
                            detail="‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ 400 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£.")
    labels = ["‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö üò°", "‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏≤‡∏á üòê", "‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å üòÑ"]
    result = []

    # Predict sentiment for each text
    for text in texts_to_process:
        sentiment_idx, prob = predict_sentiment(text)
        
        sentiment = labels[sentiment_idx]
        confidence = round(max(prob[0]) * 100, 2)
        prob_object = []
        for i in range(len(prob[0])):
            label = labels[i]
            prob_object.append(Probabilities(label=label, probability=round(prob[0][i], 3)))
        
        summary = f"‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {confidence}% ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°{sentiment}"
        result.append(SentimentResponse(text=text, sentiment=sentiment, probabilities=prob_object, summary=summary))

    return MultipleSentimentResponse(result=result)

@router.post("/predict_file")
async def predict_file(file: UploadFile = File(...)):    
    """
    Upload .txt or .csv file and get sentiment analysis for each line or row.
    """
    try:
        texts = await extract_text_from_file(file)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    
    if not texts:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏ì‡∏ú‡∏•‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå")
    
    request = MultipleSentimentRequest(texts=texts)

    return predict_multiple(request)


@router.post("/youtube")
def sentiment_from_youtube(data: YouTubeRequest):
    """
    Fetch comments from a YouTube video and predict sentiment for each comment.
    """
    comments = fetch_comments(data.url, limit=250)
    if not comments:
        raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ")
    
    request = MultipleSentimentRequest(texts=comments)
    return predict_multiple(request)